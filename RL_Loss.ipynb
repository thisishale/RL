{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2020,
     "status": "ok",
     "timestamp": 1572770690617,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "1fpLNXcNlRLL",
    "outputId": "d118d3ff-a18a-49aa-d113-e5bce1fc2b6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "# tf.enable_eager_execution()\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiplicativeLR, StepLR, LambdaLR\n",
    "import h5py\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from natsort import natsorted\n",
    "print('imported')\n",
    "# #######################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution! change foldername before execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1572770696598,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "K0kyLLwfqmZJ",
    "outputId": "a6079220-b1dc-4601-f0e7-9f96a79746de"
   },
   "outputs": [],
   "source": [
    "# Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = 'D:/hale'\n",
    "Data_path = '/content/drive/My Drive/RL_loss'\n",
    "Data_path = os.getcwd()[0:-5]\n",
    "ckpt_folder = '1'\n",
    "\n",
    "checkpoint_path = os.path.normpath(os.path.join(Data_path,'results','checkpoints',ckpt_folder))\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "files = glob.glob(os.path.normpath(os.path.join(checkpoint_path,'*')))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "loss_path = os.path.normpath(os.path.join(Data_path,'results','losses',ckpt_folder))\n",
    "if not os.path.exists(loss_path):\n",
    "    os.makedirs(loss_path)\n",
    "files = glob.glob(os.path.normpath(os.path.join(loss_path,'*')))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "time_path = os.path.normpath(os.path.join(Data_path,'results','times',ckpt_folder))\n",
    "if not os.path.exists(time_path):\n",
    "    os.makedirs(time_path)\n",
    "files = glob.glob(os.path.normpath(os.path.join(time_path,'*')))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "    \n",
    "timestep = 16\n",
    "batch_size = 32\n",
    "n_features = 257\n",
    "epochs_num =100\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_loader():\n",
    "\n",
    "    def __init__(self, path, batch_size, timestep, X_, Y_):\n",
    "        self.write_path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.len_data = 0\n",
    "        self.X_ = X_\n",
    "        self.Y_ = Y_\n",
    "        self.timestep = timestep\n",
    "        with h5py.File(os.path.normpath(os.path.join(self.write_path))+'.hdf5', \"r\") as f:\n",
    "            obj = f['X']\n",
    "            self.len_data = obj.shape[0]\n",
    "            self.n_features = obj.shape[1]\n",
    "        self.index_arr = np.arange(self.len_data)\n",
    "        self.index_arr = self.index_arr[0:(len(self.index_arr)//self.timestep)*self.timestep]\n",
    "        self.index_arr = self.index_arr.reshape((len(self.index_arr)//self.timestep,self.timestep))\n",
    "        np.random.shuffle(self.index_arr)\n",
    "\n",
    "    def check_if_left(self):\n",
    "        if len(self.index_arr)>=self.batch_size:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_batch(self):\n",
    "        self.X_out=self.X_[self.index_arr[0:self.batch_size].reshape(1,self.batch_size*self.timestep)]\n",
    "        self.Y_out=self.Y_[self.index_arr[0:self.batch_size].reshape(1,self.batch_size*self.timestep)]\n",
    "        self.index_arr = self.index_arr[self.batch_size:]\n",
    "        return self.X_out.reshape(self.batch_size,self.timestep,self.n_features),self.Y_out.reshape(self.batch_size,self.timestep,self.n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.normpath(os.path.join(Data_path,'h5_files_e','30hdata_2','features','data_30h_loge'))\n",
    "path_val = os.path.normpath(os.path.join(Data_path,'h5_files_e','devdata_3','features','devdata_30h_loge'))\n",
    "files = 'data_30h_loge'\n",
    "files_val = 'devdata_30h_loge'\n",
    "with h5py.File(path+'.hdf5', \"r\") as f:\n",
    "    X = f['X'][:]\n",
    "with h5py.File(path+'.hdf5', \"r\") as f:\n",
    "    Y = f['Y'][:]\n",
    "with h5py.File(path_val+'.hdf5', \"r\") as f:\n",
    "    X_val = f['X'][:]\n",
    "with h5py.File(path_val+'.hdf5', \"r\") as f:\n",
    "    Y_val = f['Y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_batch = Dataset_loader( path, batch_size, timestep, X, Y)\n",
    "load_batch.check_if_left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features,hidden_size=257, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(514, 257)\n",
    "    def forward(self, x):\n",
    "        x, _  = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "net.cuda()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x[0], x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_old(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_DQN1(r1, r2, w, w1p, w2p, optim_q):\n",
    "    wlps1 = w[0] + alpha*(r1+(gamma*w1p))\n",
    "    wlps2 = w[1] + alpha*(r2+(gamma*w2p))\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(wlps1, wlps2)\n",
    "    optim_q.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_q.step()\n",
    "optim_q = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = np.loadtxt('mel40.txt')\n",
    "for i in range(40):\n",
    "    plt.plot(mel[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, w1, w2, fbank):\n",
    "        batch_size = y_true.shape[0]\n",
    "        n_features = y_true.shape[-1]\n",
    "        true = torch.sqrt(torch.exp(y_true))\n",
    "        pred = torch.sqrt(torch.exp(y_pred))\n",
    "        fbank= fbank.reshape((1,fbank.shape[0],fbank.shape[1]))\n",
    "        fbank = np.repeat(fbank, batch_size, axis=0)\n",
    "        filtered_true = torch.einsum('mnp,mqp->mnq', true, torch.tensor(fbank,device=torch.device('cuda:0')))\n",
    "        filtered_pred = torch.einsum('mnp,mqp->mnq', pred, torch.tensor(fbank,device=torch.device('cuda:0')))\n",
    "#         print(torch.mean(torch.pow(self.filtered_true-self.filtered_pred,2)))\n",
    "#         print(torch.mean(torch.pow(self.y_true-self.y_pred,2)))\n",
    "        l1 = torch.mean(torch.pow(filtered_true-filtered_pred,2))*w1\n",
    "        l2 = torch.mean(torch.pow(y_true-y_pred,2))*w2\n",
    "        l = (torch.mean(torch.pow(filtered_true-filtered_pred,2))*w1)+(w2*torch.mean(torch.pow(y_true-y_pred,2)))\n",
    "        return l, l1, l2\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_loss(l1,l2):\n",
    "    memory_size = 50\n",
    "    if len(l1_memory) >= memeory_size:\n",
    "        l1_memory.append(l1)\n",
    "        l2_memory.append(l2)\n",
    "    else:\n",
    "        l1_memory.insert(0,l1_memory.pop())\n",
    "        l2_memory.insert(0,l2_memory.pop())\n",
    "        l1_memory[0] = l1\n",
    "        l2_memory[0] = l2\n",
    "    al1 = np.mean(np.stack(l1))\n",
    "    al2 = np.mean(np.stack(l2))\n",
    "    return al1, al2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loss = np.zeros((epochs_num,1))\n",
    "val_loss = np.zeros((epochs_num,1))+np.inf\n",
    "start = time.time()\n",
    "criterion = custom_losses\n",
    "running_loss = 0.0\n",
    "loss_tot = 0.0\n",
    "v_loss_tot = 0.0\n",
    "i=0\n",
    "w_1 = 0.5\n",
    "w_2 = 0.5\n",
    "net.train()\n",
    "while load_batch.check_if_left():\n",
    "    inputs, labels = load_batch.get_batch()\n",
    "    inputs=Variable(torch.tensor(inputs).float())\n",
    "    labels=Variable(torch.tensor(labels).float())\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    criterion = custom_losses\n",
    "    loss, l_1, l_2 = criterion(outputs, labels, w_1, w_2, mel)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    loss_tot += loss.item()\n",
    "    if (i+1) % 1000 == 0:   \n",
    "        print('[epoch %d, minibatch %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / 1000))\n",
    "        running_loss = 0.0\n",
    "    i = i+1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs_num): \n",
    "    running_loss = 0.0\n",
    "    loss_tot = 0.0\n",
    "    v_loss_tot = 0.0\n",
    "    i=0\n",
    "    load_batch = Dataset_loader(path, batch_size, timestep, X, Y)\n",
    "    load_batch_val = Dataset_loader(path_val, batch_size, timestep, X_val, Y_val)\n",
    "    w_1 = 0.5\n",
    "    w_2 = 0.5\n",
    "\n",
    "    net.train()\n",
    "    while load_batch.check_if_left():\n",
    "        \n",
    "        inputs, labels = load_batch.get_batch()\n",
    "        \n",
    "        inputs=Variable(torch.tensor(inputs).float())\n",
    "        labels=Variable(torch.tensor(labels).float())\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        criterion = custom_losses\n",
    "        loss, l_1, l_2 = criterion(outputs, labels, w_1, w_2, mel)\n",
    "        al_1, al_2 = store_loss(l_1, l_2)\n",
    "        w_1_p, w_2_p = DQN(l_1, l_2)\n",
    "        loss_p, l_1_p, l_2_p = criterion(outputs, labels, w_1, w_2, mel)\n",
    "        r_1 = l_1_p - al_1\n",
    "        r_2 = l_2_p - al_2\n",
    "        w_1_p, w_2_p = DQN(l_1_p, l_2_p)\n",
    "        optim_DQN1(r_1, r_2, w_1, w_2, w_1_p, w_2_p)\n",
    "        w_1 = w_1_p\n",
    "        w_2 = w_2_p\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        loss_tot += loss.item()\n",
    "        if (i+1) % 1000 == 0:   \n",
    "            print('[epoch %d, minibatch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "        i = i+1\n",
    "    train_loss[epoch] = loss_tot/i\n",
    "    print('epoch %d, train loss: %.3f' %(epoch+1, loss_tot/i))\n",
    "    net.eval()\n",
    "    i=0\n",
    "    while load_batch_val.check_if_left():\n",
    "        inputs_val, labels_val = load_batch_val.get_batch()\n",
    "        inputs_val=Variable(torch.tensor(inputs_val).float())\n",
    "        labels_val=Variable(torch.tensor(labels_val).float())\n",
    "        inputs_val = inputs_val.cuda()\n",
    "        labels_val = labels_val.cuda()\n",
    "        outputs_val = net(inputs_val)\n",
    "        criterion = custom_losses(outputs_val, labels_val, epoch)\n",
    "        v_loss = criterion()\n",
    "        v_loss_tot += v_loss.item()\n",
    "        i=i+1\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "#     scheduler.step()\n",
    "    if (v_loss_tot/i)<np.min(val_loss):\n",
    "        torch.save(net.state_dict(), os.path.normpath(os.path.join(checkpoint_path,'ckpt_'+str(epoch+1)+'.pth')))\n",
    "    val_loss[epoch] = v_loss_tot/i\n",
    "    print('************ epoch %d, val loss: %.3f ************' %(epoch+1, v_loss_tot/i))\n",
    "print('Finished Training')\n",
    "stop = time.time()\n",
    "with open((os.path.normpath(os.path.join(time_path,str(ckpt_folder)+'.txt'))), 'w') as f:\n",
    "  f.write('%d' % int(stop-start))\n",
    "np.savetxt(os.path.normpath(os.path.join(loss_path,'train_loss.txt')),train_loss)\n",
    "np.savetxt(os.path.normpath(os.path.join(loss_path,'val_loss.txt')),val_loss)\n",
    "model_path = os.path.normpath(os.path.join(Data_path,'results','models',str(ckpt_folder)+'.pth'))\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print('Finished saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss.data.cpu().numpy().reshape(1,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "du16_2_rnn_mi.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
